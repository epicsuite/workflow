# EPIC workflow

This workflow transforms `fastq` files into time-based 4D datasets that can be viewed in a 
specific visualization browser.

- [input deck specification](input.md) This is the input deck required by the end-to-end
  workflow. It defines all input files and metadata needed to create the 4D data for
  the rest of the workflow.
- [EDGE end-to-end workflow](https://github.com/epicsuite/workflow/tree/main/nextflow) 
  This is the NextFlow-based workflow that takes the input
  deck and transforms it into 4D datasets that can be viewed with an interactive viewer.
- [4D data specification](https://github.com/epicsuite/episcope/blob/main/spec/1.1.md)
  This is the specification for the 4D datasets required by downstream analysis and 
  visualization tools.
- [4D dataset viewer](https://github.com/epicsuite/episcope) This is the interactive
  viewer that can browse 4D datasets.

## Step 1: Data Upload and Workflow Definition

1. User uploads an `experimental_design.csv` file, which describes the datasets
   to be compared. This workflow requires `[1, ..., n]` lines with pairs of
   logically associated files. The assumption is that each line describes an
   identical timestep in a pair of datasets. These timesteps are assumed to be 
   in the order provided in `step 3`.

```
filepath_1,filepath_2,description
/path/to/filename0.0.fastq,/path/to/filename1.0.fastq,description of pair to be compared
/path/to/filename0.1.fastq,/path/to/filename1.1.fastq,description of pair to be compared
...
/path/to/filename0.N.fastq,/path/to/filename1.N.fastq,description of pair to be compared

```
2. User uploads `fastq` files required by the `experimental_design.csv` file.
   The data files are moved to the `build/fastq/` directory. 
3. User defines high level attributes of the workflow. This is saved in a `workflow.init` file:

```
cell_line:   name
description: some description
experiment:  name
replicate:   1
resolution:  100000
timeunits:   hr
timevalues:  [24, 48, ..., N]
treatments:  [one, two]
```

4. Using information from `1,2,3` the `workflow.yaml` file is created:

```
cell_line:   name
datasets:                           (generated by workflow)
  -   - /path/to/filename0.0.fastq
      - /path/to/filename0.1.fastq
      - ...
      - /path/to/filename0.N.fastq
  -   - /path/to/filename1.0.fastq
      - /path/to/filename1.1.fastq
      - ...
      - /path/to/filename1.N.fastq
description: some description
experiment:  name
replicate:   1
resolution:  100000
timeunits:   hr
timevalues:  [24, 48, ..., N]
treatments:  [one, two]
version:     x.x                    (automatically added by the workflow)
```


### Requirements

1. There is one `.fastq` file per dataset per timestep.
1. All `.fastq` files contain date for the same list of chromosomes.
2. All `.fastq` files contain data for the same resolution. 
3. All datasets have the same number of timesteps, and those timesteps have the
   same `timevalues`.

## Step 2: FastQ-to-HiC processing

1. Using the artifacts from `step 1`, run `SLURPy` to produce one `.hic` file
   per `.fastq` file. The new `.hic` files are created in `build/hic`
   directory.
2. The files shall be named `build/hic/d(number).t(number).hic` where
   `d(number)` is the dataset number and `t(number)` is the timestep number.
   The dataset and timestep number are taken from the order defined in the
   `workflow.yaml` file. The first list under `datasets` in the yaml file
   provides the dataset ID, and the list under the dataset defines the 
   timestep ID.


## Step 3: HiC to Structure step

1. For each `fastq` file in the `workflow.yaml` file, run `hic-to-structure` to
   produce a structure file for each chromosome in
   `build/chrN/(dataset)/(timestep)`. The list of chromosomes shall be
   automatically extracted from the `.fastq` files.

### Note

1. At this step, the workflow branches, because each `.fastq/.hic` data input
   results in `n` structure files being created (one per chromosome present in
   the files). Once completed, any of these structure files can move on to the
   next step.
2. We need to define what must be captured in the `hic-to-structure` file. 

## Step 4: Data upload step

1. Track data for a specific chromosome is added to the correct chromosome
   build directory. There will be `2 x numtimesteps x numtracks + 1 (feature
   file)` files uploaded per chromosome. The [target
   viewer](https://github.com/epicsuite/epicview/tree/main/compare) for this
   workflow shall define the tracks required for this step.

2. Metadata for this step is captured in a `vis-data-fusion.yaml` which is
   saved in `build/chrN/` directory. This defines required input tracks, and
   what the tracks can be used for. The [target
   viewer](https://github.com/epicsuite/epicview/tree/main/compare) shall
   define the contents of this file.

3. Run the next step to create vis files.

### Requirements

1. Each dataset has a complete set of track files, as expected by the
   visualization application. 

## Step 5: Vis Data Fusion step

1. For a specific Chromosome, use files in the `build/chrN` directory and
   create data in the `results/chrN` directory. This is done by iterating over
   the datasets and timesteps in the source directory and creating files in the
   `results` directory.

